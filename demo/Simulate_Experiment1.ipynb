{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "#import both RL and Foraing agents \n",
    "from Agent import QAgent   \n",
    "from Agent import ForagingAgent\n",
    "\n",
    "#generate 2-armed bandit environment \n",
    "from Walks import KArmedBandit as ab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best parameters (that maximize reward) for both models in bandit environement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting agents and environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walks parameters \n",
    "\n",
    "n = 300 \n",
    "k = 2 \n",
    "\n",
    "ab.n= n # number of trials per simulation\n",
    "ab.k = k # number of arms \n",
    "ab.step_size = 0.1 # probability step size \n",
    "ab.hazard_rate = 0.1 #hazard rate \n",
    "ab.lb = 0.1 #lower bound of reward probability\n",
    "ab.hb = 0.9 #higher bound of reward probability\n",
    "\n",
    "#agents parameters \n",
    "\n",
    "alpha_bnd = (0,1) #learning rate\n",
    "beta_bnd = (0, 100) #inverse temperature\n",
    "rho_bnd = (-1.5,1.5) #threshold (only in foraging model)\n",
    "\n",
    "#\n",
    "epsilon = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(agent,theta, walks): \n",
    "\n",
    "    if agent == \"QL\": \n",
    "        alpha, beta = theta\n",
    "        agent = QAgent(walks.shape[0],walks.shape[1],walks,alpha=alpha,beta=beta)\n",
    "        \n",
    "    elif agent == \"FOR\":\n",
    "        alpha, beta, rho = theta\n",
    "        agent = ForagingAgent(walks.shape[0],walks.shape[1],walks,alpha=alpha,beta=beta, rho=rho)\n",
    "\n",
    "    agent.walk()\n",
    "    agent_rewards = agent.get_reward_history()\n",
    "    agent_values = agent.get_q_history()\n",
    "    agent_choices = agent.get_choice_history()\n",
    "\n",
    "    # p(objective best choice)\n",
    "    best_choices = np.argmax(walks, axis=0)\n",
    "    equal_prob = (walks[0, :] == walks[1, :])\n",
    "    best_choices[equal_prob] = agent_choices[equal_prob]\n",
    "    p_best = np.mean(agent_choices == best_choices)\n",
    "\n",
    "    # p ( normalize rwd )\n",
    "\n",
    "    p_rwdNorm = (agent_rewards.mean() /  np.mean(walks)) - 1 \n",
    "\n",
    "    return p_best,  p_rwdNorm\n",
    "\n",
    "\n",
    "def linear_function(x, slope, intercept):\n",
    "    return slope * x + intercept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization step\n",
    "\n",
    "Here we find the best parameters for each models that maximize the overall reward. \n",
    "To plot the performance of the agents, run the \"plotAgentPerf\" notebook with the generated data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [02:23<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "agents = [\"QL\", \"FOR\"]\n",
    "\n",
    "replication = 258 # we match the number of participants in experiment 1\n",
    "\n",
    "\n",
    "sim_perf = {agent: [] for agent in agents}\n",
    "\n",
    "for h in tqdm(range (replication)):\n",
    "    \n",
    "    alpha =  random.uniform(*alpha_bnd)\n",
    "    beta = random.uniform(*beta_bnd)\n",
    "    rho = random.uniform(*rho_bnd)\n",
    "\n",
    "    walks, _ = ab.generate_walk(ab, plot=False,plt_title=None)\n",
    "    \n",
    "    for agent in agents : \n",
    "        \n",
    "        if agent == \"QL\":\n",
    "            \n",
    "            bnds = (alpha_bnd,beta_bnd )\n",
    "            theta = [alpha,beta]\n",
    "\n",
    "\n",
    "            theta_optim = minimize(lambda theta : - (sim(agent,theta,walks)[1]), theta, \n",
    "                                   method = \"TNC\", bounds = bnds, tol = 0.001)\n",
    "                               \n",
    "            \n",
    "    \n",
    "            alpha, beta = theta_optim.x\n",
    "            p_best, rwd = sim(agent,[alpha,beta],walks)\n",
    "\n",
    "            df = pd.DataFrame( { \"succ\" : [theta_optim.success], \n",
    "             \"nfev\" : [theta_optim.nfev], \n",
    "             \"nit\" : [theta_optim.nit], \n",
    "             \"alpha\": [alpha] , \"beta\": [beta], \"fun\": [theta_optim.fun], \n",
    "             \"rwd\" : [rwd], \"p_best\" : [p_best] }) \n",
    "            \n",
    "            sim_perf[agent].append(df)\n",
    "        \n",
    "        if agent == \"FOR\":\n",
    "            \n",
    "            bnds = (alpha_bnd,beta_bnd,rho_bnd )\n",
    "            theta = [alpha,beta,rho]\n",
    "\n",
    "\n",
    "            theta_optim = minimize(lambda theta : - (sim(agent,theta,walks)[1]), theta, \n",
    "                                   method = \"TNC\", bounds = bnds,tol = 0.001)\n",
    "                                \n",
    "            \n",
    "    \n",
    "            alpha, beta, rho = theta_optim.x\n",
    "            p_best, rwd = sim(agent,[alpha,beta,rho],walks)\n",
    "\n",
    "            df = pd.DataFrame( { \"succ\" : [theta_optim.success], \n",
    "             \"nfev\" : [theta_optim.nfev], \n",
    "             \"nit\" : [theta_optim.nit], \n",
    "             \"alpha\": [alpha] , \"beta\": [beta], \"rho\": [rho], \"fun\": [theta_optim.fun], \n",
    "             \"rwd\" : [rwd], \"p_best\" : [p_best] }) \n",
    "\n",
    "            sim_perf[agent].append(df)\n",
    "\n",
    "\n",
    "\n",
    "for agent in agents:\n",
    "    sim_perf[agent] = pd.concat(sim_perf[agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bestperfDic.pickle', 'wb') as file:\n",
    "    pkl.dump(sim_perf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "748a24a7e12f043e255937764f69584c10b2b974a22ed84d52db2de81e21ea53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
